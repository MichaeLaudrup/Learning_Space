{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "336831f5",
   "metadata": {},
   "source": [
    "## Conceptos previos\n",
    "Un LLM es un tipo de inteligencia artificial que puede generar y comprender texto humano. Los LLMs se entrenan en enormes conjuntos de datos de texto y código, lo que les permite aprender las reglas del lenguaje y cómo usarlas para generar texto de forma significativa.\n",
    "\n",
    "**Modelos**: se refieren a los modelos de lenguaje grande (LLM) que sustentan gran parte de su funcionamiento\n",
    "\n",
    "**Los prompts** son instrucciones que se dan a un LLM para indicarle qué tipo de texto debe generar. Un prompt puede ser tan simple como una frase o tan complejo como un párrafo entero. Por ejemplo, podrías darle a un LLM el prompt \"Escribe un poema sobre un gato\" o \"Escribe un artículo de noticias sobre la última investigación sobre el cambio climático\".\n",
    "\n",
    "**Los parsers**: se utilizan para analizar la salida de un LLM y convertirla en un formato más estructurado. Por ejemplo, un parser podría tomar la salida de un LLM que es un poema y dividirlo en estrofas y versos. O un parser podría tomar la salida de un LLM que es un artículo de noticias y extraer los datos clave, como el titular, la fecha y el autor.\n",
    "\n",
    "normalmente cuando se entrena un modelo LLM se cumple el mismo patrón de hacer un promt, ejecutar inferencia del modelo de LLM y luego parsear la respuesta del modelo, de está manera nacio **LangChain** que intenta crear un entorno de trabajo que permita abstraer todo el proceso y simplificarlo.\n",
    "\n",
    "## LangChain\n",
    "\n",
    "Tal y como se comento anteriormente LangChain es un marco de trabajo que facilita la construcción de aplicaciones con LLMs proporcionando una serie de componentes que se pueden combinar para crear cadenas de operaciones complejas. Una cadena típica podría incluir un paso para solicitar un modelo, un paso para analizar la salida y un paso para guardar los resultados en una base de datos.\n",
    "\n",
    "| Componente | Descripción |\n",
    "|------------|-------------|\n",
    "| Modelos    | LangChain proporciona una biblioteca de modelos LLM preentrenados que se pueden utilizar para una variedad de tareas. |\n",
    "| Formatters | Los formatters se utilizan para parsear la salida de los modelos LLM en un formato más estructurado. |\n",
    "| Pipelines  | Las pipelines son secuencias de componentes que se pueden combinar para crear cadenas de operaciones complejas. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea220356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\mlluis\\appdata\\local\\anaconda3\\lib\\site-packages (0.28.1)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\mlluis\\appdata\\local\\anaconda3\\lib\\site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mlluis\\appdata\\local\\anaconda3\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\mlluis\\appdata\\local\\anaconda3\\lib\\site-packages (from openai) (3.8.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mlluis\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mlluis\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mlluis\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mlluis\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mlluis\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->openai) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mlluis\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->openai) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\mlluis\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mlluis\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mlluis\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mlluis\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mlluis\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm->openai) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\mlluis\\appdata\\local\\anaconda3\\lib\\site-packages\\huggingface_hub-0.17.3-py3.8.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\n"
     ]
    }
   ],
   "source": [
    "#!pip install python-dotenv\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff438bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'OPENAI_API_KEY'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv, find_dotenv\n\u001b[0;32m      5\u001b[0m _ \u001b[38;5;241m=\u001b[39m load_dotenv(find_dotenv()) \u001b[38;5;66;03m# read local .env file\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m openai\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m<frozen os>:679\u001b[0m, in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'OPENAI_API_KEY'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "os.environ['OPENAI_API_KEY'] = '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'\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b8d1d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

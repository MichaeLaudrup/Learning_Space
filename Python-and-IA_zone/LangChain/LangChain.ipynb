{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "336831f5",
   "metadata": {},
   "source": [
    "# Super resumen comandos de este notebook\n",
    "\n",
    "| Comando | descripcion |\n",
    "|:---|:---|\n",
    "| ``from langchain.prompts import ChatPromptTemplate`` | Importamos clase para crear plantilla de prompt |\n",
    "| ``template_string = 'hazme un resumen del libro {titulo_libro}'`` | Creamos una string base para el prompt donde lo que esta entre llaves indica variables del prompt |\n",
    "| ``prompt_template = ChatPromptTemplate.from_template(template_string)`` | Creamos una plantilla base a partir del anterior  |\n",
    "| ``template_filled = prompt_template.format_messages(titulo_libro=\"Harry potter\")`` | Instanciamos una plantilla de prompt con determinados valores |\n",
    "| ``from langchain.chat_models import ChatOpenAI`` | Creamos nuestro modelo LLM de tipo chat, en este caso usaremos el de openAI |\n",
    "|`` chat(template_filled)``| Invocamos al modelo de chat insertandole el prompt |\n",
    "|``from langchain.output_parsers import ResponseSchema``| Importamos la clase que nos sirve para esquematizar la salida|\n",
    "|``from langchain.output_parsers import ResponseSchema``| Importamos la clase que nos sirve para esquematizar la salida|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac10034",
   "metadata": {},
   "source": [
    "## Conceptos previos\n",
    "Un LLM es un tipo de inteligencia artificial que puede generar y comprender texto humano. Los LLMs se entrenan en enormes conjuntos de datos de texto y código, lo que les permite aprender las reglas del lenguaje y cómo usarlas para generar texto de forma significativa.\n",
    "\n",
    "**Modelos**: se refieren a los modelos de lenguaje grande (LLM) que sustentan gran parte de su funcionamiento\n",
    "\n",
    "**Los prompts** son instrucciones que se dan a un LLM para indicarle qué tipo de texto debe generar. Un prompt puede ser tan simple como una frase o tan complejo como un párrafo entero. Por ejemplo, podrías darle a un LLM el prompt \"Escribe un poema sobre un gato\" o \"Escribe un artículo de noticias sobre la última investigación sobre el cambio climático\".\n",
    "\n",
    "**Los parsers**: se utilizan para analizar la salida de un LLM y convertirla en un formato más estructurado. Por ejemplo, un parser podría tomar la salida de un LLM que es un poema y dividirlo en estrofas y versos. O un parser podría tomar la salida de un LLM que es un artículo de noticias y extraer los datos clave, como el titular, la fecha y el autor.\n",
    "\n",
    "normalmente cuando se entrena un modelo LLM se cumple el mismo patrón de hacer un promt, ejecutar inferencia del modelo de LLM y luego parsear la respuesta del modelo, de está manera nacio **LangChain** que intenta crear un entorno de trabajo que permita abstraer todo el proceso y simplificarlo.\n",
    "\n",
    "## LangChain\n",
    "\n",
    "Tal y como se comento anteriormente LangChain es un marco de trabajo que facilita la construcción de aplicaciones que puedan explotar los LLMs proporcionando una serie de componentes que se pueden combinar para crear cadenas de operaciones complejas. Una cadena típica podría incluir un paso para solicitar un modelo, un paso para analizar la salida y un paso para guardar los resultados en una base de datos.\n",
    "\n",
    "| Componente | Descripción |\n",
    "|------------|-------------|\n",
    "| Modelos    | LangChain proporciona una biblioteca de modelos LLM preentrenados que se pueden utilizar para una variedad de tareas. |\n",
    "| Formatters | Los formatters se utilizan para parsear la salida de los modelos LLM en un formato más estructurado. |\n",
    "| Pipelines  | Las pipelines son secuencias de componentes que se pueden combinar para crear cadenas de operaciones complejas. |\n",
    "\n",
    "## Instalacion de lang chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea220356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Nombre_Paquete</th>\n",
       "      <th>Versión</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>langchain</td>\n",
       "      <td>0.0.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>pandas</td>\n",
       "      <td>1.5.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cb4d01-38dc-479c-bdde-c0d3b00746af",
   "metadata": {},
   "source": [
    "## Seleccion del modelo \n",
    "lanchaing ofrece abstracciones de modelos conocidos de tipo LLM (large language model), modelos de chat o text embedding Models, estos ultimos convierten un token, sentencias o texto en vectores numericos que capturan información semántica y relaciones. semanticas. En el siguiente ejemplo se vera como utilizamos un modelo de chat popular, que es el de openai, aunque hay miles de modelos más que se pueden utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff438bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, temperature=0.0, openai_api_key='Hay que asignar una key aquí', openai_api_base='', openai_organization='', openai_proxy='')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'Hay que asignar una key aquí'\n",
    "llm_model = \"gpt-3.5-turbo\"\n",
    "# To control the randomness and creativity of the generated\n",
    "# text by an LLM, use temperature = 0.0\n",
    "\n",
    "# Internamente buscara en nuestro sistema operativo \n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f62c0",
   "metadata": {},
   "source": [
    "## Creación de un promp template\n",
    "\n",
    "En el siguiente segmento de codigo se apreciará como se crea un template base que sirve como prompt para el modelo, es como una especie de plantilla, que no servirá para hacer peticiones similares con diferentes valores segun el caso que se requiera, por ejemplo, si queremos crear una aplicación cuya función principal es hacer resumenes de libros, vemos que el hecho de hacer un resumen es algo común, pero el libro especifico que se desea resumir varía según el titulo de libro que se provea, otro ejemplo es el expuesto a continuación: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94b8d1d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 40\u001b[0m\n\u001b[0;32m     34\u001b[0m customer_messages \u001b[38;5;241m=\u001b[39m prompt_template\u001b[38;5;241m.\u001b[39mformat_messages(\n\u001b[0;32m     35\u001b[0m                     style\u001b[38;5;241m=\u001b[39mcustomer_style,\n\u001b[0;32m     36\u001b[0m                     text\u001b[38;5;241m=\u001b[39mcustomer_email)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Call the LLM to translate to the style of the customer message\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m customer_response \u001b[38;5;241m=\u001b[39m chat(customer_messages)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(customer_response\u001b[38;5;241m.\u001b[39mcontent)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chat' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Como apreciamos la manera de decirle a lang chain que \n",
    "# es lo que varia segun la ejecucion es utilizando las llaves\n",
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\"\n",
    "\n",
    "# creamos una plantilla de prompt a partir de la \n",
    "# cadena de texto anterior\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "\n",
    "# Creamos variables para una instancia concreta de la \n",
    "# plantilla\n",
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\"\n",
    "\n",
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "\n",
    "# Aqui creamos una especia de instancia de la plantilla \n",
    "# del prompt utilizando keyword similares a los nombres\n",
    "# asignados mas arriba entre llaves\n",
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=customer_style,\n",
    "                    text=customer_email)\n",
    "\n",
    "\n",
    "# Call the LLM to translate to the style of the customer message\n",
    "customer_response = chat(customer_messages)\n",
    "\n",
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd61d211",
   "metadata": {},
   "source": [
    "## Parsear la salida\n",
    "\n",
    "Una vez tenemos definido nuestro modelo, nuestro prompt template, lo siguiente es definir como queremos que nos llegue la respuesta, el LLM nos dará una salida general y nuestro objetivo es que nos devuelva la información esquematizada o estructurada de una determinada manera para luego nosotros poder trabajar con ella. A continuación se muestra un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35edac7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'customer_review' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 48\u001b[0m\n\u001b[0;32m     28\u001b[0m review_template_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124mFor the following text, extract the following information:\u001b[39m\n\u001b[0;32m     30\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;132;01m{format_instructions}\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     46\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(template\u001b[38;5;241m=\u001b[39mreview_template_2)\n\u001b[1;32m---> 48\u001b[0m messages \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mformat_messages(text\u001b[38;5;241m=\u001b[39mcustomer_review, \n\u001b[0;32m     49\u001b[0m                                 format_instructions\u001b[38;5;241m=\u001b[39mformat_instructions)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'customer_review' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "gift_schema = ResponseSchema(name=\"gift\",\n",
    "                             description=\"Was the item purchased\\\n",
    "                             as a gift for someone else? \\\n",
    "                             Answer True if yes,\\\n",
    "                             False if not or unknown.\")\n",
    "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
    "                                      description=\"How many days\\\n",
    "                                      did it take for the product\\\n",
    "                                      to arrive? If this \\\n",
    "                                      information is not found,\\\n",
    "                                      output -1.\")\n",
    "price_value_schema = ResponseSchema(name=\"price_value\",\n",
    "                                    description=\"Extract any\\\n",
    "                                    sentences about the value or \\\n",
    "                                    price, and output them as a \\\n",
    "                                    comma separated Python list.\")\n",
    "\n",
    "response_schemas = [gift_schema, \n",
    "                    delivery_days_schema,\n",
    "                    price_value_schema]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "review_template_2 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "messages = prompt.format_messages(text=customer_review, \n",
    "                                format_instructions=format_instructions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

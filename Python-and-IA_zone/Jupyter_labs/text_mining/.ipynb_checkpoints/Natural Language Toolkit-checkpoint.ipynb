{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6793bd3",
   "metadata": {},
   "source": [
    "# Natural Language Toolkit (NLTK)\n",
    "Natural Language Toolkit (NLTK) es una biblioteca open source para Python especializada en ofrecer herramientas para tareas de procesamiento de lenguaje natural (NLP). La NLP abarca tareas que involucran computación o manipulación del lenguaje natural, desde operaciones básicas (contar palabras) hasta funciones avanzadas (¿Que sentimiento causa un texto?). A continuación, algunos conceptos fundamentales:\n",
    "\n",
    "- **Tokenización**: Proceso de dividir un texto en palabras, frases, símbolos u otros elementos significativos (tokens) para un análisis más fácil.\n",
    "- **Tokens**: Unidades individuales resultantes del proceso de tokenización. Pueden ser palabras o signos de puntuación.\n",
    "- **Sentencias**: Conjuntos de palabras que expresan una idea completa, típicamente separadas por puntos u otros signos de puntuación.\n",
    "- **Lematización**: Proceso de reducir las palabras a su forma base o raíz, considerando el análisis morfológico.\n",
    "- **POS Tagging (Etiquetado gramatical)**: Asignación de etiquetas a cada token en un texto basado en su definición y contexto.\n",
    "- **Parsing (Análisis sintáctico)**: Proceso de analizar un texto para determinar su estructura gramatical.\n",
    "- **NER (Reconocimiento de Entidades Nombradas)**: Identificación y clasificación de entidades nombradas presentes en el texto, como nombres de personas, organizaciones, lugares, etc.\n",
    "\n",
    "Además, NLTK ofrece herramientas y recursos para una amplia gama de tareas, como el análisis de sentimientos, identificación de entidades en una sentencia (sujeto, objeto, verbo), determinación de referencias pronominales, entre otros.\n",
    "\n",
    "### Text corpora\n",
    "En el contexto de NLTK, un \"corpus\" (plural \"corpora\") se refiere a un conjunto de textos (libros, documentos, etc). NLTK es una herramienta popular en el campo del Procesamiento del Lenguaje Natural y proporciona acceso a muchos corpora y recursos léxicos, junto con bibliotecas para procesar texto, realizar análisis lingüístico y más.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9d83b39",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importamos la librería nltk\n",
    "import nltk\n",
    "\n",
    "\"\"\"\n",
    "    Al hacer nltk.download('book'), estás descargando todos los datasets\n",
    "    y recursos asociados con el libro \"Natural Language Processing with Python\" \n",
    "    que utiliza nltk para ejemplos y ejercicios. \n",
    "\"\"\" \n",
    "# quiet=True evita un log enorme indicando como se va descargando todos los elementos\n",
    "nltk.download('book', quiet=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ccb8e5",
   "metadata": {},
   "source": [
    "## Text corpora Gutenberg\n",
    "\n",
    "El Project Gutenberg es una colección digital de libros electrónicos gratuitos, principalmente de obras literarias clásicas que se encuentran en el dominio público. Es uno de los esfuerzos digitales más antiguos para hacer accesibles a todos obras literarias, y contiene miles de libros en varios idiomas. Fue fundado por Michael S. Hart en 1971.\n",
    "\n",
    "El corpus de Project Gutenberg en el contexto de nltk (y en muchos otros contextos de procesamiento del lenguaje natural) se refiere a un subconjunto de textos seleccionados de la colección completa del Project Gutenberg. Estos textos se utilizan frecuentemente en investigaciones y educación debido a su disponibilidad y naturaleza libre de derechos.\n",
    "\n",
    "El corpora (conjunto de textos) del Projecto Gutenberg esta compuesto por varios textos famosos como\n",
    "<ul>\n",
    "    <li>text1: Moby Dick by Herman Melville 1851\n",
    "   <li>text2: \"Sense and Sensibility\" de Jane Austen, 1811\n",
    "   <li>text3: \"The Book of Genesis\"\n",
    "   <li>text4: \"Inaugural Address Corpus\" - una colección de discursos presidenciales de Estados Unidos desde 1789-2009\n",
    "   <li>text5: \"NPS Chat\" - una colección de mensajes de chat del Internet Relay Chat\n",
    "   <li>text6: \"Monty Python and the Holy Grail\" - un guión de película de comedia\n",
    "   <li>text7: \"Wall Street Journal\" - una selección de artículos del periódico\n",
    "   <li>text8: \"Personals Corpus\" - anuncios personales\n",
    "   <li>text9: \"The Man Who Was Thursday by G. K. Chesterton 1908\"\n",
    "</ul>\n",
    "\n",
    "Cuando nos descargamos la colección \"book\" nos descargamos todos estos textos\n",
    "\n",
    "### Textos \n",
    "\n",
    "La representación de estos textos en nltk es mediante objetos complejos de tipo `Text` del módulo `text` de `nltk`, que entre otras cosas tiene:\n",
    "\n",
    "- **Variables**:\n",
    "  - `name`: El nombre o título del texto.\n",
    "  - `tokens`: Una lista de las palabras o tokens del texto.\n",
    "\n",
    "- **Métodos**:\n",
    "  - `concordance(word)`: Muestra las ocurrencias de una palabra junto con su contexto.\n",
    "  - `similar(word)`: Encuentra palabras que aparecen en contextos similares al de la palabra dada.\n",
    "  - `common_contexts(words)`: Muestra los contextos compartidos por dos o más palabras.\n",
    "  - `dispersion_plot(words)`: Crea un gráfico que muestra la distribución de palabras en el texto.\n",
    "  - `vocab()`: Devuelve un objeto `FreqDist` que representa la distribución de frecuencia de palabras en el texto.\n",
    "  - `count(word)`: Cuenta las ocurrencias de una palabra en el texto.\n",
    "\n",
    "Estas son solo algunas de las funcionalidades ofrecidas por el objeto `Text` de `nltk`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68e586c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1:  <Text: Moby Dick by Herman Melville 1851>\n",
      "El tipo de text1 es un objeto complejo de tipo Text: <class 'nltk.text.Text'>\n",
      "====================\n",
      "Text 8:  <Text: Personals Corpus>\n"
     ]
    }
   ],
   "source": [
    "# Una vez descargado el libro obtenemos todas sus variables\n",
    "# funciones y elementos\n",
    "from nltk.book import *\n",
    "\n",
    "#corpus de Project Gutenberg\n",
    "print('Text 1: ',text1)\n",
    "print('El tipo de text1 es un objeto complejo de tipo Text:', type(text1) )\n",
    "print('====================')\n",
    "print('Text 8: ', text8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57bd881",
   "metadata": {},
   "source": [
    "### Sentencias (sent1, sent2,..., sent9)\n",
    "Otro elemento de un corpora en el contexto de la librería nlkt son las sentencias, de cada texto se ha extraído una sentencia para que los principiantes puedan trabajar con ellas sin tener que procesar grandes volumenes de texto al entrar en contacto con la librería. Veamos el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14178230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentencia del texto 1 ['Call', 'me', 'Ishmael', '.']\n",
      "Sentencia del texto 2 ['The', 'family', 'of', 'Dashwood', 'had', 'long', 'been', 'settled', 'in', 'Sussex', '.']\n"
     ]
    }
   ],
   "source": [
    "print('Sentencia del texto 1', sent1)\n",
    "print('Sentencia del texto 2', sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7ab3f5",
   "metadata": {},
   "source": [
    "### Conteo básico de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56e29d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de palabras de la sentencia 1:  4\n",
      "Número de tokens (palabras y signos de puntuación) en el texto:  260819\n",
      "Número de tokens unicos (que no se repiten):  19317\n",
      "Obtener las diez primeras palabras de ese conjunto unico ['Bartholomew', 'wavings', 'bluer', 'overstocked', 'term', 'justice', 'HOLLAND', 'plunging', 'assertion', 'belayed']\n"
     ]
    }
   ],
   "source": [
    "# Podemos contar palabras de una sentencia utilizando la funcion len\n",
    "print('Numero de palabras de la sentencia 1: ', len(sent1))\n",
    "print('Número de tokens (palabras y signos de puntuación) en el texto: ', len(text1) )\n",
    "print('Número de tokens unicos (que no se repiten): ', len(set(text1)))\n",
    "print('Obtener las diez primeras palabras de ese conjunto unico', list(set(text1))[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9584740",
   "metadata": {},
   "source": [
    "#### Frequencia de palabras\n",
    "\n",
    "La frecuencia de palabras se refiere al número de veces que una palabra aparece en un texto determinado. Analizar la frecuencia de palabras puede proporcionar información significativa sobre el contenido y la naturaleza de un texto. Por ejemplo, en un artículo sobre economía, es probable que palabras como \"mercado\", \"inversión\" o \"finanzas\" tengan una alta frecuencia.\n",
    "\n",
    "En el contexto de nltk, el objeto FreqDist es una clase para la distribución de frecuencia de palabras en un texto. Es, en esencia, una especialización de un diccionario en Python que mapea cada palabra a su frecuencia de aparición.\n",
    "\n",
    "En la salida que proporcionaste:\n",
    "\n",
    "    samples: Se refiere al número de eventos únicos o distintos que se han observado. En este contexto, \"samples\" representa palabras únicas en el texto. Por ejemplo, si \"economía\" aparece en el texto, independientemente de cuántas veces lo haga, se considera una \"muestra\".\n",
    "\n",
    "    outcomes: Se refiere al total de eventos observados, es decir, la suma de todas las frecuencias. En términos de procesamiento de texto, \"outcomes\" es el número total de palabras (o tokens) en el texto, incluidas las repeticiones.\n",
    "\n",
    "En tu caso, el objeto FreqDist para text7 indica que hay 12,408 palabras únicas (samples) en el texto y un total de 100,676 palabras en general (outcomes).\n",
    "\n",
    "Si quieres explorar más a fondo, puedes usar métodos del objeto FreqDist para obtener las palabras más comunes, la frecuencia de una palabra específica, entre otros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "59ceedbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frecuencia de palabras en un texto número 7:  <FreqDist with 12408 samples and 100676 outcomes>\n",
      "\n",
      "¿Cuales son las diez palabras más comunes en el texto? (palabra, frecuencia):  [(',', 4885), ('the', 4045), ('.', 3828), ('of', 2319), ('to', 2164), ('a', 1878), ('in', 1572), ('and', 1511), ('*-1', 1123), ('0', 1099)]\n",
      "\n",
      "Obtener los tokens sin repetir del texto:  ['Pierre', 'Vinken', ',', '61', 'years', 'old', 'will', 'join', 'the', 'board']\n",
      "\n",
      "¿Frecuencia de la palabra \"million\" es? :  383\n",
      "\n",
      "Obtener tokens que cumplan que su longitud sea mayor que 5 y su frecuenca mayor que 100: \n",
      "['billion', 'company', 'president', 'because', 'market', 'million', 'shares', 'trading', 'program']\n"
     ]
    }
   ],
   "source": [
    "distribucion_frequencia = FreqDist(text7)\n",
    "print('Frecuencia de palabras en un texto número 7: ', distribucion_frequencia )\n",
    "print('\\n¿Cuales son las diez palabras más comunes en el texto? (palabra, frecuencia): ',distribucion_frequencia.most_common(10))\n",
    "unique_tokens = list(distribucion_frequencia.keys())\n",
    "print('\\nObtener los tokens sin repetir del texto: ', unique_tokens[:10])\n",
    "print('\\n¿Frecuencia de la palabra \"million\" es? : ', distribucion_frequencia[\"million\"])\n",
    "print('\\nObtener tokens que cumplan que su longitud sea mayor que 5 y su frecuenca mayor que 100: ')\n",
    "freqPalabrasCondicionada = [w for w in unique_tokens if len(w) > 5 and distribucion_frequencia[w] > 100]\n",
    "print(freqPalabrasCondicionada)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00244185",
   "metadata": {},
   "source": [
    "## Normalización y Stemming en Procesamiento del Lenguaje Natural\n",
    "\n",
    "### Normalización\n",
    "\n",
    "La **normalización** es el proceso de convertir un texto en una forma canónica o estándar. Esto es importante en el procesamiento del lenguaje natural (NLP, por sus siglas en inglés) ya que nos permite tratar palabras y frases en diferentes formas como si fueran las mismas. Por ejemplo, la normalización podría involucrar:\n",
    "\n",
    "- Convertir todo el texto a minúsculas: para que \"Palabra\" y \"palabra\" sean consideradas iguales.\n",
    "- Eliminar signos de puntuación.\n",
    "- Convertir números escritos en texto a su forma numérica, por ejemplo, \"dos\" a \"2\".\n",
    "- Corrección ortográfica.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217c8c15",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "El **stemming** es una técnica específica dentro de la normalización que implica reducir una palabra a su raíz o base. El objetivo es eliminar las variaciones morfológicas de una palabra (por ejemplo, eliminar sufijos y prefijos más comunes), para que diferentes formas de la misma palabra, como \"correr\", \"corredor\" y \"corriendo\", sean tratadas de la misma manera. Es importante destacar que la palabra resultante después de aplicar el stemming, llamada \"stem\", no necesariamente tiene que ser una palabra real en el lenguaje; simplemente representa la \"base\" de la palabra original.\n",
    "\n",
    "Existen diversos algoritmos y herramientas para realizar stemming, siendo el \"Stemmer de Porter\" uno de los más conocidos en el ámbito de la lengua inglesa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a249d37f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list', 'list', 'list', 'list', 'list', 'listen', 'listen']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "#Hacer minusculas y separar es una manera de normalizar\n",
    "variaciones_palabra = 'List listed lists listing listings listening listener'.lower().split(' ')\n",
    "stemmed_words = [porter.stem(palabra) for palabra in variaciones_palabra]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f649a0",
   "metadata": {},
   "source": [
    "### Lemmatization (Lematización)\n",
    "\n",
    "La **lematización** es un proceso lingüístico que implica reducir una palabra a su forma base o lema. A diferencia del **stemming**, que simplemente elimina ciertos sufijos o prefijos de una palabra sin tener en cuenta su significado, la lematización tiene en cuenta el contexto y la morfología de la palabra, lo que significa que se basa en el diccionario y el análisis morfológico de las palabras para llevar a cabo la reducción.\n",
    "\n",
    "Por ejemplo:\n",
    "- El lematizador trataría \"corriendo\" y \"corre\" como formas de la palabra base \"correr\".\n",
    "- El lema de \"mejores\" podría ser \"bueno\", ya que considera el grado del adjetivo.\n",
    "\n",
    "En el contexto de la NLP, la lematización es esencial porque ayuda a **reducir la dimensionalidad del texto** y a agrupar diferentes formas de una palabra, de modo que puedan ser analizadas como una sola entidad. Esto es especialmente útil en tareas como la clasificación de texto, análisis de sentimientos, y búsqueda de información.\n",
    "\n",
    "NLTK ofrece una herramienta llamada `WordNetLemmatizer` que es capaz de lematizar palabras teniendo en cuenta su categoría gramatical y utilizando la base de datos léxica de WordNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0d26a54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras Stemetizadas:  ['univers', 'declar', 'of', 'human', 'right', 'preambl', 'wherea', 'recognit', 'of', 'the', 'inher', 'digniti', 'and', 'of', 'the', 'equal', 'and', 'inalien', 'right', 'of']\n",
      "** Se aprecia que no todas son palabras validas\n",
      "\n",
      "________________________________________________\n",
      "\n",
      "Palabras Lematizadas:  ['Universal', 'Declaration', 'of', 'Human', 'Rights', 'Preamble', 'Whereas', 'recognition', 'of', 'the', 'inherent', 'dignity', 'and', 'of', 'the', 'equal', 'and', 'inalienable', 'right', 'of']\n",
      "** Se aprecia que Todas son palabras validas\n"
     ]
    }
   ],
   "source": [
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "\n",
    "udhr = nltk.corpus.udhr.words('English-Latin1')\n",
    "primeras_veinte_palabras = udhr[:20]\n",
    "palabras_stemmed = [porter.stem(t) for t in primeras_veinte_palabras]\n",
    "print('Palabras Stemetizadas: ', palabras_stemmed)\n",
    "print('** Se aprecia que no todas son palabras validas')\n",
    "print('\\n________________________________________________\\n')\n",
    "palabras_lematizadas = [WNlemma.lemmatize(t) for t in primeras_veinte_palabras]\n",
    "print('Palabras Lematizadas: ', palabras_lematizadas)\n",
    "print('** Se aprecia que Todas son palabras validas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76622fe8",
   "metadata": {},
   "source": [
    "### Tokenization (Tokenización)\n",
    "\n",
    "La **tokenización** es el proceso de dividir un texto en unidades más pequeñas, llamadas tokens. Estos tokens pueden representar palabras, frases o incluso signos de puntuación. En la mayoría de los contextos de NLP, se considera que un token es una palabra individual en un texto.\n",
    "\n",
    "La tokenización es uno de los primeros y más esenciales pasos en el preprocesamiento de cualquier tarea de NLP. Antes de poder analizar o interpretar un texto, es fundamental dividirlo en componentes manejables. \n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "    La frase \"El perro corre rápidamente.\" se tokenizaría en: [\"El\", \"perro\", \"corre\", \"rápidamente\", \".\"]\n",
    "\n",
    "Existen diferentes enfoques y técnicas para la tokenización, y la elección de una u otra puede depender de la naturaleza del lenguaje, la estructura del texto y la tarea específica de NLP que se esté llevando a cabo.\n",
    "\n",
    "En NLTK, hay varias funciones para tokenizar texto, siendo `word_tokenize` una de las más utilizadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c9d77b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['El', 'perro', 'corre', 'rápidamente', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "texto = \"El perro corre rápidamente.\"\n",
    "tokens = word_tokenize(texto)\n",
    "\n",
    "print('Tokens:', tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ff3088",
   "metadata": {},
   "source": [
    "### Sentence Splitting (División de Sentencias)\n",
    "\n",
    "El **sentence splitting** o división de sentencias es el proceso de segmentar un texto en sus sentencias constituyentes. Es un paso crucial en el preprocesamiento de texto en muchas tareas de NLP, ya que muchas veces es necesario trabajar con unidades de información que representen pensamientos o ideas completas, es decir, las sentencias.\n",
    "\n",
    "La división de sentencias puede parecer una tarea sencilla a primera vista; podríamos sentirnos tentados de simplemente dividir un texto cada vez que encontramos un punto. Sin embargo, esta estrategia es propensa a errores. Considera los siguientes escenarios:\n",
    "\n",
    "- Abreviaciones como \"Dr.\", \"Sr.\", \"U.S.A.\", donde el punto no indica el final de una sentencia.\n",
    "- Uso de puntos en números, como \"3.5\" o fechas como \"23.04.2022\".\n",
    "- Sentencias que terminan con signos de interrogación (?) o exclamación (!).\n",
    "\n",
    "Dada la complejidad inherente a la división precisa de sentencias, es fundamental utilizar herramientas diseñadas específicamente para esta tarea.\n",
    "\n",
    "En NLTK, el `sent_tokenize` es una función que utiliza un algoritmo preentrenado para segmentar texto en sentencias, teniendo en cuenta las complejidades y excepciones antes mencionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "33b59fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentencias: ['Dr. Smith se graduó en la U.S.A. en 2022.', '¿Sabías eso?', '¡Es impresionante!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "texto = \"Dr. Smith se graduó en la U.S.A. en 2022. ¿Sabías eso? ¡Es impresionante!\"\n",
    "sentencias = sent_tokenize(texto)\n",
    "\n",
    "print('Sentencias:', sentencias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c84f60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
